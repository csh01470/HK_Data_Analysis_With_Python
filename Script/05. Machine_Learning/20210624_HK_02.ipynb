{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "389278cbb61c1c57cad107a3c1fdd79953aeebb4509d1d7f21aea6fe620a6af8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".. _20newsgroups_dataset:\n\nThe 20 newsgroups text dataset\n------------------------------\n\nThe 20 newsgroups dataset comprises around 18000 newsgroups posts on\n20 topics split in two subsets: one for training (or development)\nand the other one for testing (or for performance evaluation). The split\nbetween the train and test set is based upon a messages posted before\nand after a specific date.\n\nThis module contains two loaders. The first one,\n:func:`sklearn.datasets.fetch_20newsgroups`,\nreturns a list of the raw texts that can be fed to text feature\nextractors such as :class:`~sklearn.feature_extraction.text.CountVectorizer`\nwith custom parameters so as to extract feature vectors.\nThe second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\nreturns ready-to-use features, i.e., it is not necessary to use a feature\nextractor.\n\n**Data Set Characteristics:**\n\n    =================   ==========\n    Classes                     20\n    Samples total            18846\n    Dimensionality               1\n    Features                  text\n    =================   ==========\n\nUsage\n~~~~~\n\nThe :func:`sklearn.datasets.fetch_20newsgroups` function is a data\nfetching / caching functions that downloads the data archive from\nthe original `20 newsgroups website`_, extracts the archive contents\nin the ``~/scikit_learn_data/20news_home`` folder and calls the\n:func:`sklearn.datasets.load_files` on either the training or\ntesting set folder, or both of them::\n\n  >>> from sklearn.datasets import fetch_20newsgroups\n  >>> newsgroups_train = fetch_20newsgroups(subset='train')\n\n  >>> from pprint import pprint\n  >>> pprint(list(newsgroups_train.target_names))\n  ['alt.atheism',\n   'comp.graphics',\n   'comp.os.ms-windows.misc',\n   'comp.sys.ibm.pc.hardware',\n   'comp.sys.mac.hardware',\n   'comp.windows.x',\n   'misc.forsale',\n   'rec.autos',\n   'rec.motorcycles',\n   'rec.sport.baseball',\n   'rec.sport.hockey',\n   'sci.crypt',\n   'sci.electronics',\n   'sci.med',\n   'sci.space',\n   'soc.religion.christian',\n   'talk.politics.guns',\n   'talk.politics.mideast',\n   'talk.politics.misc',\n   'talk.religion.misc']\n\nThe real data lies in the ``filenames`` and ``target`` attributes. The target\nattribute is the integer index of the category::\n\n  >>> newsgroups_train.filenames.shape\n  (11314,)\n  >>> newsgroups_train.target.shape\n  (11314,)\n  >>> newsgroups_train.target[:10]\n  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n\nIt is possible to load only a sub-selection of the categories by passing the\nlist of the categories to load to the\n:func:`sklearn.datasets.fetch_20newsgroups` function::\n\n  >>> cats = ['alt.atheism', 'sci.space']\n  >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n\n  >>> list(newsgroups_train.target_names)\n  ['alt.atheism', 'sci.space']\n  >>> newsgroups_train.filenames.shape\n  (1073,)\n  >>> newsgroups_train.target.shape\n  (1073,)\n  >>> newsgroups_train.target[:10]\n  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n\nConverting text to vectors\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn order to feed predictive or clustering models with the text data,\none first need to turn the text into vectors of numerical values suitable\nfor statistical analysis. This can be achieved with the utilities of the\n``sklearn.feature_extraction.text`` as demonstrated in the following\nexample that extract `TF-IDF`_ vectors of unigram tokens\nfrom a subset of 20news::\n\n  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n  >>> categories = ['alt.atheism', 'talk.religion.misc',\n  ...               'comp.graphics', 'sci.space']\n  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n  ...                                       categories=categories)\n  >>> vectorizer = TfidfVectorizer()\n  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n  >>> vectors.shape\n  (2034, 34118)\n\nThe extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\ncomponents by sample in a more than 30000-dimensional space\n(less than .5% non-zero features)::\n\n  >>> vectors.nnz / float(vectors.shape[0])\n  159.01327...\n\n:func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which \nreturns ready-to-use token counts features instead of file names.\n\n.. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\n.. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\n\n\nFiltering text for more realistic training\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIt is easy for a classifier to overfit on particular things that appear in the\n20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\nhigh F-scores, but their results would not generalize to other documents that\naren't from this window of time.\n\nFor example, let's look at the results of a multinomial Naive Bayes classifier,\nwhich is fast to train and achieves a decent F-score::\n\n  >>> from sklearn.naive_bayes import MultinomialNB\n  >>> from sklearn import metrics\n  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n  ...                                      categories=categories)\n  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n  >>> clf = MultinomialNB(alpha=.01)\n  >>> clf.fit(vectors, newsgroups_train.target)\n  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n\n  >>> pred = clf.predict(vectors_test)\n  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n  0.88213...\n\n(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\nthe training and test data, instead of segmenting by time, and in that case\nmultinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\nyet of what's going on inside this classifier?)\n\nLet's take a look at what the most informative features are:\n\n  >>> import numpy as np\n  >>> def show_top10(classifier, vectorizer, categories):\n  ...     feature_names = np.asarray(vectorizer.get_feature_names())\n  ...     for i, category in enumerate(categories):\n  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n  ...\n  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n  alt.atheism: edu it and in you that is of to the\n  comp.graphics: edu in graphics it is for and of to the\n  sci.space: edu it that is in and space to of the\n  talk.religion.misc: not it you in is that and to of the\n\n\nYou can now see many things that these features have overfit to:\n\n- Almost every group is distinguished by whether headers such as\n  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n- Another significant feature involves whether the sender is affiliated with\n  a university, as indicated either by their headers or their signature.\n- The word \"article\" is a significant feature, based on how often people quote\n  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n  wrote:\"\n- Other features match the names and e-mail addresses of particular people who\n  were posting at the time.\n\nWith such an abundance of clues that distinguish newsgroups, the classifiers\nbarely have to identify topics from text at all, and they all perform at the\nsame high level.\n\nFor this reason, the functions that load 20 Newsgroups data provide a\nparameter called **remove**, telling it what kinds of information to strip out\nof each file. **remove** should be a tuple containing any subset of\n``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\nblocks, and quotation blocks respectively.\n\n  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n  ...                                      remove=('headers', 'footers', 'quotes'),\n  ...                                      categories=categories)\n  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n  >>> pred = clf.predict(vectors_test)\n  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')\n  0.77310...\n\nThis classifier lost over a lot of its F-score, just because we removed\nmetadata that has little to do with topic classification.\nIt loses even more if we also strip this metadata from the training data:\n\n  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n  ...                                       remove=('headers', 'footers', 'quotes'),\n  ...                                       categories=categories)\n  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n  >>> clf = MultinomialNB(alpha=.01)\n  >>> clf.fit(vectors, newsgroups_train.target)\n  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n\n  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n  >>> pred = clf.predict(vectors_test)\n  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n  0.76995...\n\nSome other classifiers cope better with this harder version of the task. Try\nrunning :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\nthe ``--filter`` option to compare the results.\n\n.. topic:: Recommendation\n\n  When evaluating text classifiers on the 20 Newsgroups data, you\n  should strip newsgroup-related metadata. In scikit-learn, you can do this by\n  setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n  lower because it is more realistic.\n\n.. topic:: Examples\n\n   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\n\n   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n\n"
     ]
    }
   ],
   "source": [
    "#2021.06.24. THU\n",
    "#Hankyeong\n",
    "\n",
    "#00. 패키지 호출\n",
    "import warnings\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.datasets import fetch_20newsgroups \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#00-1. warning message ignore\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "#07. Newsgroup 텍스트 데이터셋을 통해 긍정/부정 분류하기.  \n",
    "#(1) 데이터셋 불러오기. \n",
    "news_raw = fetch_20newsgroups(subset='all', random_state=156)\n",
    "\n",
    "#PLUS. news 데이터셋의 설명 확인하기. \n",
    "print(news_raw.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "#PLUS. news 데이터셋의 key 확인하기. \n",
    "news_raw.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "From: egreen@east.sun.com (Ed Green - Pixel Cruncher)\nSubject: Re: Observation re: helmets\nOrganization: Sun Microsystems, RTP, NC\nLines: 21\nDistribution: world\nReply-To: egreen@east.sun.com\nNNTP-Posting-Host: laser.east.sun.com\n\nIn article 211353@mavenry.altcit.eskimo.com, maven@mavenry.altcit.eskimo.com (Norman Hamer) writes:\n> \n> The question for the day is re: passenger helmets, if you don't know for \n>certain who's gonna ride with you (like say you meet them at a .... church \n>meeting, yeah, that's the ticket)... What are some guidelines? Should I just \n>pick up another shoei in my size to have a backup helmet (XL), or should I \n>maybe get an inexpensive one of a smaller size to accomodate my likely \n>passenger? \n\nIf your primary concern is protecting the passenger in the event of a\ncrash, have him or her fitted for a helmet that is their size.  If your\nprimary concern is complying with stupid helmet laws, carry a real big\nspare (you can put a big or small head in a big helmet, but not in a\nsmall one).\n\n---\nEd Green, former Ninjaite |I was drinking last night with a biker,\n  Ed.Green@East.Sun.COM   |and I showed him a picture of you.  I said,\nDoD #0111  (919)460-8302  |\"Go on, get to know her, you'll like her!\"\n (The Grateful Dead) -->  |It seemed like the least I could do...\n\n\n"
     ]
    }
   ],
   "source": [
    "#PLUS. news 데이터셋의 텍스트 데이터 구조 확인하기. \n",
    "print(news_raw.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                    text  target\n",
       "0      From: egreen@east.sun.com (Ed Green - Pixel Cr...       8\n",
       "1      From: jlevine@rd.hydro.on.ca (Jody Levine)\\nSu...       8\n",
       "2      From: u95_dgold@vaxc.stevens-tech.edu\\nSubject...      12\n",
       "3      From: jca2@cec1.wustl.edu (Joseph Charles Achk...      10\n",
       "4      From: jonathan@comp.lancs.ac.uk (Mr J J Trevor...       6\n",
       "...                                                  ...     ...\n",
       "18841  From: brian@lpl.arizona.edu (Brian Ceccarelli ...      19\n",
       "18842  From: d12751@tanus.oz.au (Jason Bordujenko)\\nS...       3\n",
       "18843  From: rwf2@ns1.cc.lehigh.edu (ROBERT WILLIAM F...       7\n",
       "18844  From: bc@idx.com\\nSubject: Request info on a m...       3\n",
       "18845  From: paula@koufax.cv.hp.com (Paul Andresen)\\n...       9\n",
       "\n",
       "[18846 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>From: egreen@east.sun.com (Ed Green - Pixel Cr...</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>From: jlevine@rd.hydro.on.ca (Jody Levine)\\nSu...</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>From: u95_dgold@vaxc.stevens-tech.edu\\nSubject...</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>From: jca2@cec1.wustl.edu (Joseph Charles Achk...</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>From: jonathan@comp.lancs.ac.uk (Mr J J Trevor...</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>18841</th>\n      <td>From: brian@lpl.arizona.edu (Brian Ceccarelli ...</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>18842</th>\n      <td>From: d12751@tanus.oz.au (Jason Bordujenko)\\nS...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>18843</th>\n      <td>From: rwf2@ns1.cc.lehigh.edu (ROBERT WILLIAM F...</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>18844</th>\n      <td>From: bc@idx.com\\nSubject: Request info on a m...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>18845</th>\n      <td>From: paula@koufax.cv.hp.com (Paul Andresen)\\n...</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n<p>18846 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "#(2) news_raw 데이터셋 데이터프레임으로 변환하기. \n",
    "df_news_raw = pd.DataFrame({'text': news_raw.data,\n",
    "                        'target': news_raw.target})\n",
    "df_news_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "#MEMO. target 변수는 연속형이 아닌 카테고리 분류값임!\n",
    "news_raw.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 18846 entries, 0 to 18845\nData columns (total 2 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   text    18846 non-null  object\n 1   target  18846 non-null  int32 \ndtypes: int32(1), object(1)\nmemory usage: 221.0+ KB\n"
     ]
    }
   ],
   "source": [
    "#(3) 데이터셋의 결측값 확인하기. \n",
    "df_news_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0     799\n",
       "1     973\n",
       "2     985\n",
       "3     982\n",
       "4     963\n",
       "5     988\n",
       "6     975\n",
       "7     990\n",
       "8     996\n",
       "9     994\n",
       "10    999\n",
       "11    991\n",
       "12    984\n",
       "13    990\n",
       "14    987\n",
       "15    997\n",
       "16    910\n",
       "17    940\n",
       "18    775\n",
       "19    628\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "#(4) target 변수의 분포 확인하기. \n",
    "pd.Series(news_raw.target).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                    text  target\n",
       "0      \\nIf your primary concern is protecting the pa...       8\n",
       "1      I feel childish.\\n\\n\\nWho mentioned dirtbikes?...       8\n",
       "2                                                             12\n",
       "3      \\nGretzky averaged 2.69 pts/game\\n\\n\\nCheck yo...      10\n",
       "4      If anyone would like to get rid of their SegaC...       6\n",
       "...                                                  ...     ...\n",
       "18841  \\nThat's right.  Everyone.  Even infants who c...      19\n",
       "18842  G'day All,\\n\\nI was looking to build a Paralle...       3\n",
       "18843   ites:\\n Yeah, and the cop couldn't catch me.....       7\n",
       "18844  While rummaging through a box of old PC (5150)...       3\n",
       "18845  \\nIt's always possible, but if this is the cas...       9\n",
       "\n",
       "[18846 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>\\nIf your primary concern is protecting the pa...</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I feel childish.\\n\\n\\nWho mentioned dirtbikes?...</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td></td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\\nGretzky averaged 2.69 pts/game\\n\\n\\nCheck yo...</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>If anyone would like to get rid of their SegaC...</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>18841</th>\n      <td>\\nThat's right.  Everyone.  Even infants who c...</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>18842</th>\n      <td>G'day All,\\n\\nI was looking to build a Paralle...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>18843</th>\n      <td>ites:\\n Yeah, and the cop couldn't catch me.....</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>18844</th>\n      <td>While rummaging through a box of old PC (5150)...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>18845</th>\n      <td>\\nIt's always possible, but if this is the cas...</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n<p>18846 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "#(5) headers, footers, quotes를 제거해서 데이터셋 불러오기.  \n",
    "news = fetch_20newsgroups(subset='all', random_state=156, remove=('headers','footers','quotes')) \n",
    "\n",
    "#(6) 데이터 프레임화 하기. \n",
    "df_news = pd.DataFrame({'text': news.data,\n",
    "                        'target': news.target})\n",
    "df_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nIf your primary concern is protecting the passenger in the event of a\ncrash, have him or her fitted for a helmet that is their size.  If your\nprimary concern is complying with stupid helmet laws, carry a real big\nspare (you can put a big or small head in a big helmet, but not in a\nsmall one).\n\n"
     ]
    }
   ],
   "source": [
    "#PLUS. news 데이터셋의 텍스트 데이터 구조 확인하기. \n",
    "print(news.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((15076,), (3770,), (15076,), (3770,))"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "#(7) train, test 데이터셋으로 분할하기. \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_news['text'], df_news['target'], stratify=df_news['target'], test_size=0.2, random_state=2021\n",
    ")\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((15076, 122346), (3770, 122346))"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "#08. 피쳐 벡터화 변환과 머신러닝 모델 학습, 예측, 평가하기. \n",
    "#    Case I. Counter Vectorizer + Logistic Regression \n",
    "#(1) Count Vectorizer 객체 생성하기. \n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "#(2) X_train 데이터셋을 넣어 학습하기. \n",
    "count_vect.fit(X_train)\n",
    "\n",
    "#(3) transform() 메서드를 이용해 벡터화하기. \n",
    "X_train_cv = count_vect.transform(X_train)\n",
    "X_test_cv  = count_vect.transform(X_test)\n",
    "\n",
    "#(4) 산출물의 shape 확인하기.\n",
    "X_train_cv.shape, X_test_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "#(5) Logistic Regression 모델 설정하기. \n",
    "lr = LogisticRegression()\n",
    "\n",
    "#(6) 모델 학습하기. \n",
    "lr.fit(X_train_cv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.676657824933687"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "#(7) 모델의 예측, 평가하기. \n",
    "lr_pred = lr.predict(X_test_cv)\n",
    "accuracy_score(y_test,lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': None,\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'l1_ratio': None,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'auto',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'solver': 'lbfgs',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "#PLUS. 모델의 하이퍼 파라미터 확인하기. \n",
    "lr.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((15076, 122346), (3770, 122346))"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "#09. 피쳐 벡터화 변환과 머신러닝 모델 학습, 예측, 평가하기. \n",
    "#    Case II. TF-IDF Vectorzier + Logistic Regression \n",
    "#(1) tfid Vectorizer 객체 생성하기. \n",
    "tfid_vect = TfidfVectorizer()\n",
    "\n",
    "#(2) X_train 데이터셋을 넣어 학습하기. \n",
    "tfid_vect.fit(X_train)\n",
    "\n",
    "#(3) transform() 메서드를 이용해 벡터화하기. \n",
    "X_train_tf = tfid_vect.transform(X_train)\n",
    "X_test_tf  = tfid_vect.transform(X_test)\n",
    "\n",
    "#(4) 산출물의 shape 확인하기.\n",
    "X_train_tf.shape, X_test_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "#(5) Logistic Regression 모델 설정하기. \n",
    "lr = LogisticRegression()\n",
    "\n",
    "#(6) 모델 학습하기. \n",
    "lr.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7209549071618037"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "#(7) 모델의 예측, 평가하기. \n",
    "lr_pred = lr.predict(X_test_tf)\n",
    "accuracy_score(y_test,lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((15076, 1181620), (3770, 1181620))"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "#09. 피쳐 벡터화 변환과 머신러닝 모델 학습, 예측, 평가하기. \n",
    "#    Case III. TF-IDF Vectorzier(with parameter) + Logistic Regression \n",
    "#(1) tfid Vectorizer 객체 생성하기. \n",
    "tfid_vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "\n",
    "#(2) X_train 데이터셋을 넣어 학습하기. \n",
    "tfid_vect.fit(X_train)\n",
    "\n",
    "#(3) transform() 메서드를 이용해 벡터화하기. \n",
    "X_train_tf = tfid_vect.transform(X_train)\n",
    "X_test_tf  = tfid_vect.transform(X_test)\n",
    "\n",
    "#(4) 산출물의 shape 확인하기.\n",
    "X_train_tf.shape, X_test_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "#(5) Logistic Regression 모델 설정하기. \n",
    "lr = LogisticRegression()\n",
    "\n",
    "#(6) 모델 학습하기. \n",
    "lr.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7371352785145888"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "#(7) 모델의 예측, 평가하기. \n",
    "lr_pred = lr.predict(X_test_tf)\n",
    "accuracy_score(y_test,lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7615384615384615"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "#PLUS. Logistic Regression에도 하이퍼 파라미터를 부여할 경우, \n",
    "lr = LogisticRegression(C=10)\n",
    "lr.fit(X_train_tf, y_train)\n",
    "lr_pred = lr.predict(X_test_tf)\n",
    "accuracy_score(y_test,lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10. 피쳐 벡터화 변환과 머신러닝 모델 학습, 예측, 평가하기. \n",
    "#    Case IV. TF-IDF Vectorzier + Logistic Regression + GridsearchCV + Pipeline \n",
    "#(1) Pipeline 정의하기. \n",
    "pipeline = Pipeline([\n",
    "    ('tfid_vect', TfidfVectorizer(stop_words='english')),\n",
    "    ('lr', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2) 하이퍼 파라미터 설정하기. \n",
    "params = {\n",
    "    'tfid_vect__ngram_range' : [(1,1), (1,2)],\n",
    "    'tfid_vect__max_df'      : [300,700],\n",
    "    'lr__C'                  : [1,10,50,100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(3) GridsearchCV로 모델 정의하기. \n",
    "gscv_pipe = GridSearchCV(\n",
    "    pipeline, param_grid=params, cv=3,\n",
    "    scoring='accuracy', verbose=1, n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "Wall time: 17min 11s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('tfid_vect',\n",
       "                                        TfidfVectorizer(stop_words='english')),\n",
       "                                       ('lr', LogisticRegression())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'lr__C': [1, 10, 50, 100],\n",
       "                         'tfid_vect__max_df': [300, 700],\n",
       "                         'tfid_vect__ngram_range': [(1, 1), (1, 2)]},\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "#(4) 모델 학습하기. \n",
    "%time gscv_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7474801061007957"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "#(5) 모델의 예측 및 평가하기. \n",
    "gscv_pipe_fit = gscv_pipe.best_estimator_\n",
    "gscv_pipe_pred = gscv_pipe_fit.predict(X_test)\n",
    "accuracy_score(y_test,gscv_pipe_pred)"
   ]
  }
 ]
}